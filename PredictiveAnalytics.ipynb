{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## required packages\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoccerDataset(object):\n",
    "    def __init__(self):\n",
    "        self.DATA_DIR='data/SoccerData.xlsx'\n",
    "        self.sheet='All Data'\n",
    "        self.data=self.load_data()\n",
    "    #load data\n",
    "    def load_data(self):\n",
    "        data = pd.read_excel(self.DATA_DIR, self.sheet, index_col=None)#reading data in particular sheet 'All Data'\n",
    "        return data\n",
    "    \n",
    "    ##get list of column names\n",
    "    def getColumns(self):\n",
    "        return self.data.columns\n",
    "    \n",
    "    ##shows non null value caount and data type per column \n",
    "    def getDataInfo(self):\n",
    "        return self.data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SoccerDataset=SoccerDataset()\n",
    "data=SoccerDataset.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>## Data Cleansing ##</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get info about data\n",
    "SoccerDataset.getDataInfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these informations we can already see that some features won't be relevant in our exploratory analysis as there are too much missing values (such as Unnamed: 79 and Unnamed: 80). However, from our exploratory analysis, features such as BTS, H1H, A1H, H2H, A2H, teams_no has no significant role to predict result.  Plus there is so much features to analyse that it may be better to concentrate on the ones which can give us real insights. Let's just remove the features with 50% or more NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns with more than 50% values missed\n",
    "data_clean = data[[column for column in data if data[column].count() / len(data) >= 0.5]]\n",
    "print(\"List of dropped columns:\", end=\" \")\n",
    "for c in data.columns:\n",
    "    if c not in data_clean.columns:\n",
    "        print(c, end=\", \")\n",
    "print('\\n')\n",
    "data = data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets remove features with no significant dependancy to other features\n",
    "\n",
    "##One more thing is we inspect correlation matrix, date feature is less related to the other but one can further explore its impact with playing frequently or at the of the game has impact on the result or not. But for now I will drop the coloumn.\n",
    "\n",
    "From our exploratory analysis (See histogram distrbution of each features), we can see that critcal features are aroung team stats and odds feature. By skiping home_team and away_team features to game and we can create two separate data as result feature dependa on two. \n",
    "So, for now I will create the data for Home team using RESULT coulumn \"HOME\" as \"WIN\" and later I will refine the code for away team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_col_list = ['home_team','away_team','teams_no', 'h_half','a_half', 'a_final', 'h_final', 'BTS', 'H1H', \n",
    "                'date', 'A1H','H2H', 'A2H', 'league']\n",
    "\n",
    "data=data.drop(del_col_list, axis=1)\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## our data has single categorical feature RESULT after previous preprocessing step \n",
    "## but let create function for labeling categorical features\n",
    "# Categorical boolean mask. I am using lablelEncoder\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder# instantiate labelencoder object\n",
    "\n",
    "##function to encode all categorical features\n",
    "def encode_categorical_features(df):\n",
    "    lblEncoder = LabelEncoder()\n",
    "    categorical_feature_mask = df.dtypes==object# filter categorical columns using mask and turn it into a list\n",
    "    categorical_cols = df.columns[categorical_feature_mask].tolist()\n",
    "    # apply le on categorical feature columns\n",
    "    df[categorical_cols] = df[categorical_cols].apply(lambda col: lblEncoder.fit_transform(col))\n",
    "    return df\n",
    "##Again tranisform target feature RESULT\n",
    "data=encode_categorical_features(data)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duplicate columns has no impact as a feature. So, as one type of feature dimension reduction we will check for existance of duplicate column and remove it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly no duplicated column exist. Lets again check for conistant features to minimize problem at data spliting. We have two option to go with this. One is using separate feature and train independantly, the other is removing. I will go with the latter one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill missed value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing all the zeros from odds features to some meaningfull values.\n",
    "cols = ['home_odd','draw_odd','away_odd']\n",
    "#First lets change it to dataframe understandable null value\n",
    "data[cols] = data[cols].replace(0, np.nan)\n",
    "\n",
    "# Instead of droping zero filled odds feature value, I will use popular median approach\n",
    "median = data['home_odd'].median()\n",
    "data['home_odd'].fillna(median, inplace=True)\n",
    "\n",
    "median = data['draw_odd'].median()\n",
    "data['draw_odd'].fillna(median, inplace=True)\n",
    "\n",
    "median = data['away_odd'].median()\n",
    "data['away_odd'].fillna(median, inplace=True)\n",
    "\n",
    "## There is also missing value for h_goal_signal, a_goal_signal, and Ladder_signal. So I will do same thing\n",
    "median = data['h_goal_signal'].median()\n",
    "data['h_goal_signal'].fillna(median, inplace=True)\n",
    "\n",
    "median = data['a_goal_signal'].median()\n",
    "data['a_goal_signal'].fillna(median, inplace=True)\n",
    "\n",
    "median = data['Ladder_signal'].median()\n",
    "data['Ladder_signal'].fillna(median, inplace=True)\n",
    "\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I will focus on detecting and removing outliers. Because some fields are filled manually as indicated looking for outliers is important to standardize the data. First I will plot for existance (i.e, filtering out outliers based on fixed percentile values. I will use 95%) of outlier then using popular perecentage based detection mechanisim I will remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dealing with outliers\n",
    "# The following code snipate checks for existance of outliers in data based on columns\n",
    "col_names = data.columns.values\n",
    "fig, ax = plt.subplots(len(col_names), figsize=(8,200))\n",
    "for i, col_val in enumerate(col_names):\n",
    "    sns.boxplot(y=data[col_val], ax=ax[i])\n",
    "    ax[i].set_title('Box plot - {}'.format(col_val), fontsize=10)\n",
    "    ax[i].set_xlabel(col_val, fontsize=8)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above plot, black dots are outliers and as we can observe from the graph all variables except RESULT has outlier. The blue colored box is the IQR range.\n",
    "<ul><li>The interquartile range (IQR) is the range covered by the middle 50% of the data.\n",
    "    IQR = Q3 – Q1, the difference between the third and first quartiles. </li>\n",
    "<li>The first quartile (Q1) is the value such that one quarter (25%) of the data points fall below it, or the median of the bottom half of the data. </li>\n",
    "<li>The third quartile is the value such that three quarters (75%) of the data points fall below it, or the median of the top half of the data.</li>\n",
    "<li>The IQR can be used to detect outliers using the 1.5(IQR) criteria. Outliers are observations that fall below Q1 – 1.5(IQR) or above Q3 + 1.5(IQR).</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Percentage based outlier removal.\n",
    "def percentile_based_outlier(data, threshold=95):\n",
    "    diff = (100 - threshold) / 2\n",
    "    minval, maxval = np.percentile(data, [diff, 100 - diff])\n",
    "    return (data < minval) | (data > maxval)\n",
    "\n",
    "col_names = data.columns.values ##\n",
    "\n",
    "fig, ax = plt.subplots(len(col_names), figsize=(8,200))\n",
    "\n",
    "for i, col_val in enumerate(col_names):\n",
    "    x = data[col_val][:1000]\n",
    "    sns.distplot(x, ax=ax[i], rug=True, hist=False)\n",
    "    outliers = x[percentile_based_outlier(x)]\n",
    "    ax[i].plot(outliers, np.zeros_like(outliers), 'ro', clip_on=False)\n",
    "\n",
    "    ax[i].set_title('Outlier detection - {}'.format(col_val), fontsize=10)\n",
    "    ax[i].set_xlabel(col_val, fontsize=8)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values marked with a dot below in the x-axis of the graph are the ones that are removed from the column based on the set threshold percentile (95 in our case), and is also the default value when it comes to percentile-based outlier removal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I will apply data balacing per class levels. As we have observed at exploratory analysis data target class are partially imbalanced. So, I will use popular Over-sampling followed by under-sampling using SMOTE. Instead of affecting only over-sampled or under-sampled class, using Over-sampling followed by under-sampling approach good in handling data imbalance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Balancing\n",
    "# Function to plot class distribution \n",
    "def plot_smote(X,y):\n",
    "    # scatter plot, dots colored by class value\n",
    "    from pandas import DataFrame\n",
    "    from matplotlib import pyplot\n",
    "    colors = {0:'red', 1:'blue', 2:'green'}\n",
    "    df = DataFrame(dict(x=X[:,0], y=X[:,1], label=Y))\n",
    "    ig, ax = pyplot.subplots()\n",
    "    grouped = df.groupby('label')\n",
    "    for key, group in grouped:\n",
    "        group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])\n",
    "    pyplot.show()\n",
    "    # split data into X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets first shuffle our data. Shuffling enables us to control data leakage.\n",
    "from sklearn.utils import shuffle\n",
    "data = shuffle(data)\n",
    "y = data.RESULT\n",
    "X = data.drop(['RESULT'], axis=1)\n",
    "from imblearn.combine import SMOTETomek\n",
    "smt = SMOTETomek(ratio='auto')\n",
    "X, Y = smt.fit_sample(X, y)\n",
    "plot_smote(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets split data to train, test, and validation. I will follow what is ordered. training (60%), validation(30%) and test(10%) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data then split data into train and test sets\n",
    "scaler = Normalizer().fit(X)\n",
    "X = scaler.transform(X)\n",
    "seed = 7\n",
    "test_size = 0.10\n",
    "validation_size=0.30\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
    "X_train,X_valid,y_train,y_valid=train_test_split(X_train, y_train, test_size=validation_size, random_state=seed)\n",
    "\n",
    "print('Training Split x:{},y:{}'.format(X_train.shape[0],y_train.shape[0]))\n",
    "print('Test Split x:{},y:{}'.format(X_test.shape[0],y_test.shape[0]))\n",
    "print('Validation Split x:{},y:{}'.format(X_valid.shape[0],y_valid.shape[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I will create different models using different ALgorthms. I will run 5 fold cross validation for all algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from imblearn.pipeline import make_pipeline as make_pipeline_imb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import time\n",
    "\n",
    "def randomForest(X_train, X_test, y_train, y_test):\n",
    "  \n",
    "    # parameters = {'n_estimators': [10, 20, 30, 50], 'max_depth': [2, 3, 4]}\n",
    "\n",
    "    clf = RandomForestClassifier(max_depth=4, n_estimators=20)\n",
    "    # clf = GridSearchCV(alg, parameters, n_jobs=4)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print('RandomForest Classier')\n",
    "    print(\"Score: \", clf.score(X_test, y_test))\n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_pred_proba = clf.predict_proba(X_test)[:, 1]\n",
    "    print(\"F1 score is: {}\".format(f1_score(y_test, y_pred,average='micro')))\n",
    "\n",
    "def neural_nets(X_train, X_test, y_train, y_test):\n",
    "    print('MLP\\n')\n",
    "    clf = MLPClassifier(hidden_layer_sizes=(100, 100, 100,))\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(\"Score: \", clf.score(X_test, y_test))\n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_pred_proba = clf.predict_proba(X_test)[:, 1]\n",
    "    print(\"F1 score is: {}\".format(f1_score(y_test, y_pred,average='micro')))\n",
    "def xgb_classifier(X_train, X_test, y_train, y_test):\n",
    "    xgb_model = xgb.XGBClassifier(objective=\"multi:softprob\", random_state=42)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    y_pred = xgb_model.predict(X_train)\n",
    "    print(confusion_matrix(y, y_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomForest(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final comment\n",
    "Because of time limitation I couln't further analysed the result. As we obserbed from exploratory analysis step, most features are not highely correlated. This makes the model challeged to generalize prediction. However, I believe if we merge with some merging operation for very related features such as ladder or  odds together, it may show new accuracy. Moreover, model for teams paled at away is also not futher investigated. Hope, I will share optmized model soon through my GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
