{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ultimate goal is to minimize student frustration and satisfaction and to most effectively use limited coaching resources. Cancelling early may be one indication of frustration or low satisfaction and the more students enrolled in the course who do not make at least one payment, much less finish the course, the less coaching resources are being used effectively. \n",
    "With this in mind, in order to consider launching the experiment either of the following must be observed:\n",
    "<ul type='square'><li>Increased retention (more students staying beyond the free trial in the experiment group)</li>\n",
    "<li>Decreased Gross Conversion coupled to increased Net Conversion (less students enrolling in the free trial but more students staying beyond the free trial)</li></ul>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Total of 37 days experment and control data observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing value columns: Enrollment and Payment\n",
    "<ul><li>Enrollment:14 and Payment: 14 for Control</li>\n",
    "    <li>Enrollment:14 and Payment: 14 for Experment</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assessment of the statistical significance of an A/B test is dependent on what kind of probability distribution the experimental data follows. Given your answer above, which statistical tests are appropriate to use for this project?<br>\n",
    "<b>Answer</b><br>\n",
    "probability distribution for our control group is binomial because the we have only  two possible outcomes eaither retainion or leaving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In frequentist analysis, mostly used for A/B testing, we use p-values to measure the significance of the experimental feature over the null hypothesis (the hypothesis that the new feature does not have an impact). How are p-values computed? What information do p-values provide? Are you familiar with type-I and type-II errors? Can you comment to which error types p-values are related?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are the number of data points in the experiment enough to make a reasonable judgement or should Udacity run a longer experiment? Remember that running the experiment longer may be costly for many reasons, so you should always optimize the number of samples to make a statistically sound decision.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does your A/B testing analysis tells you? Does the experimental feature improve Enrollment, the target variable? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a analyst desires a higher power, bigger sample sizes are needed. However, compaies with less customer may cost more to collect enough sample due to elongated data collection. There are online sample size calculators such as "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Let's place baseline for estimators into a dictionary for ease of use later\n",
    "baseline = {\"Cookies\":40000,\"Clicks\":3200,\"Enrollments\":660,\"CTP\":0.08,\"GConversion\":0.20625,\n",
    "           \"Retention\":0.53,\"NConversion\":0.109313}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we collected these estimates, we should estimate the standard deviation of a metric, this is computed for sample size calculations and confidence intervals for our results. The more variant a metric is, the harder it is to reach a significant result. Assuming a sample size of 5,000 cookies visiting the course overview page per day (as given in project's instructions) - we want to estimate a standard deviation, for the evaluation metrics only. The sample size we are considering should be smaller than the \"population\" we collected and small enough to have two groups with that size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An experiment was conducted in which potential Udacity students were diverted by cookie into two groups, experiment and control. The experiment group was asked to input the amount of time they are willing to devote to study, after clicking a \"start free trial button\", whereas the control group was not. Three invariant metrics (Number of Cookies, Number of clicks on \"start free trial\", and Click-Through-Probability) were chosen for purposes of validation and sanity checking while Gross Conversion (enrollment/cookie) and Net Conversion (payments/cookie) served as evaluation metrics. The null hypothesis is that there is no difference in the evaluation metrics between the groups, futhermore, a practical signifcance threshold was set for each metric. The requirement for launching the experiment is that the null hypothesis must be rejected for ALL evaluation metrics and that the difference between branches must meet or exceed the practical signficance threshold. Because our acceptance criteria requires statiscally signifcant differences for ALL evaluation metrics, the use of the Bonferonni correction is not appropriate. The Bonferonni correction is a method for controlling for type I errors (false positives) when using multiple metrics in which relevance of ANY of the metrics matches the hypothesis. In this case the risk of type I errors increases as the number of metrics increases (signifcance by random chance). In our case in which ALL metrics must be relevant to launch, the risk of type II errors (false negatives) increases as the number of metrics increases, so it stands to reason that controlling for false positives is not consistent with our acceptance criteria.\n",
    "Analysis revealed the expected equal distribution of cookies into the control and experimental groups, for the invariant metrics, at the 95% CI. A difference in gross conversion was found to be statistically signficant at the 95% CI, and the null hypothesis was rejected. Gross conversion also met the practical signficance threshold. Net Conversion was found to be neither statistically nor practically signficant at the 95% CI.\n",
    "Recommendation\n",
    "This experiment was designed to determine whether filtering students as a function of study time commitment would improve the overall student experience and the coaches' capacity to support students who are likely to complete the course, without significantly reducing the number of students who continue past the free trial. A statistically and practically signficant decrease in Gross Conversion was observed but with no significant differences in Net Conversion. This translates to a decrease in enrollment not coupled to an increase in students staying for the requisite 14 days to trigger payment. Considering this, my recomendation is not to launch, but rather to pursue other experiments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
